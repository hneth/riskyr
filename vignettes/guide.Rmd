---
title: "`riskyr` User Guide"
author: "Hansjörg Neth"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette: 
    fig_caption: yes
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

<!-- Introduction: --> 

> "Solving a problem simply means representing it so as to make the solution transparent."<br>
> (H.A. Simon)^[Simon, H.A. (1996). _The Sciences of the Artificial_ (3rd ed.). The MIT Press, Cambridge, MA. (p. 132).]

What is the likelihood of a disease or clinical condition given a positive test result? 
This seems a common and simple question, yet doctors, patients and medical students find it surprisingly difficult to answer. 

Decades of research on probabilistic reasoning and risk literacy have shown that people struggle when information is expressed in terms of probabilities, but have no problem to process the same information when it is expressed in terms of natural frequencies (see Gigerenzer and Hoffrage, 1995; Gigerenzer et al., 2007; Hoffrage et al., 2015; for overviews).

`riskyr` is a toolbox for rendering risk literacy more transparent by facilitating such changes in representation. This guide first illustrates a typical problem and then helps you solving it by viewing risk-related information in a variety of ways that makes it easier to understand the dynamic interplay of frequencies and probabilities.


## Motivation

A basic motivation for developing `riskyr` was to facilitate our understanding of problems like the following:

> The probability of breast cancer is 1% for a woman at age 40 who participates in routine screening.  
> If a woman has breast cancer, the probability is 80% that she will get a positive mammography.  
> If a woman does not have breast cancer, the probability is 9.6% that she will also get a positive mammography.  
> 
> A woman in this age group had a positive mammography in a routine screening. What is the probability that she actually has breast cancer?    
> 
> (Hoffrage et al., 2015, p. 3)  

### Information provided and asked

Problems like this tend to appear in texts and tutorials on risk literacy and are ubiquitous in medical diagnostics. 
They typically provide some risk-related information (i.e., specific probabilities of some clinical condition and likelihoods of some decision or test of detecting its presence or absence) and ask for some other risk-related quantity. In the most basic type of scenario, we are given 3 essential probabilities: 

1. The _prevalence_ of some target population (here: women at age 40) for some condition (breast cancer):  

  - `prev` = $p(\mathrm{cancer}) = 1\%$

2. The _sensitivity_ of some decision or diagnostic procedure 
(here: a mammography screening test), which is the conditional probability:  

  - `sens` = $p(\mathrm{positive\ test}\ |\ \mathrm{cancer}) = 80\%$

3. The _false alarm rate_ of this decision, diagnostic procedure or test, which is the conditional probability:   

  - `fart` = $p( \mathrm{positive\ test}\ |\ \mathrm{no\ breast\ cancer} ) = 9.6\%$

and can also be expressed by its complement (aka. the test's _specificity_):  

  - `spec` = 1 -- `fart` = $p( \mathrm{negative\ test}\ |\ \mathrm{no\ cancer} ) = 90.4\%$  

The first challenge in solving this problem is to realize that the probability asked for is _not_ the sensitivity `sens` (i.e., the probability of a positive test given cancer), but the _reversed_ conditional probability (i.e., the probability of having cancer given a positive test). The clinical term for this quantity is the _positive predictive value_ (`PPV`) or the test's _precision_: 

  - `PPV` = $p( \mathrm{cancer}\ |\ \mathrm{positive\ test} )$ = ? 

How can we compute the _positive predictive value_ (`PPV`) from the information provided by the problem? In the following, we sketch three different paths to the solution.


### Using Bayes' formula

One way to solve problems concerning conditional probabilities is to remember and apply Bayes' formula (which is why such problems are often called problems of "Bayesian reasoning"):

$$ p(H|D) = \frac{p(H) \cdot p(D|H) } {p(H) \cdot p(D|H) + p(\neg H) \cdot p(D|\neg H) } $$


In our example, we are looking for the probability of breast cancer ($H$) given a positive mammography test ($D$):   

$$ p(\mathrm{cancer}\ |\ \mathrm{positive\ test}) 
= \frac{p(\mathrm{cancer}) \cdot p(\mathrm{positive\ test}\ |\ \mathrm{cancer}) } 
{p(\mathrm{cancer})     \cdot p(\mathrm{positive\ test}\ |\ \mathrm{cancer}) + 
 p(\mathrm{no\ cancer}) \cdot p(\mathrm{positive\ test}\ |\ \mathrm{no\ cancer}) } $$

By inserting the probabilities identified above and knowing that the probability for the absence of breast cancer in our target population is the complementary probability of its presence (i.e., $p(\mathrm{no\ cancer}) = 1 - $`prev` = 99\%$) we obtain:

$$ p(\mathrm{cancer}\ |\ \mathrm{positive\ test}) 
= \frac{1\% \cdot 80\% } 
{  1\% \cdot 80\% + 
  99\% \cdot 9.6\% } \approx\ 7.8\%$$

Thus, the information above and a few basic mathematical calculations tell us that the likelihood of a woman in our target population with a positive mammography screening test actually having breast cancer (i.e., the `PPV` of this mammography screening test) is slightly below 8\%. 

### Using natural frequencies

If you fail to find the Bayesian solution easy and straightforward, you are in good company: Even people who have studied and taught statistics find it difficult to think in these terms. Fortunately, researchers have found that a simple change in representation renders the same information much more transparent. 

Consider the following description:

> 10 out of every 1000 women at age 40 who participate in routine screening have breast cancer.   
> 8 out of every 10 women with breast cancer will get a positive mammography.  
> 95 out of every 990 women without breast cancer will also get a positive mammography.   
> 
> Here is a new representative sample of women at age 40 who got a positive mammography in a routine screening. How many of these women do you expect to actually have breast cancer?  
> 
> (Hoffrage et al., 2015, p. 4) 

Importantly, this version of the problem refers to a sample of $1000$ individuals of our original target population. It then provides the same probabilities as above, but specifies them in terms of _natural frequencies_ (see Hoffrage et al., 2002, for a clarification of this term):

1. The _prevalence_ of breast cancer in the target population:  

  - `prev` = $p(\mathrm{cancer}) = \frac{10}{1000} (= 1\%)$
  
2. The _sensitivity_ of the mammography screening test, which is the conditional probability:  

  - `sens` = $p(\mathrm{positive\ test}\ |\ \mathrm{cancer}) = \frac{8}{10} (= 80\%)$

3. The test's _false alarm rate_, which is the conditional probability:

  - `fart` = $p( \mathrm{positive\ test}\ |\ \mathrm{no\ breast\ cancer} ) = \frac{95}{990} (\approx\ 9.6\%)$

and can still be expressed by its complement (the test's _specificity_):  

  - `spec` = 1 -- `fart` = $p( \mathrm{negative\ test}\ |\ \mathrm{no\ cancer} ) = \frac{990 - 95}{990} = \frac{895}{990} (\approx\ 90.4\%)$  

Rather than asking us to compute a conditional probability (i.e., the `PPV`), the task now prompts us to imagine a new representative sample of women from our target population and focuses on the women with a positive test result. It then asks for a _frequency_: "How many of these women" do we expect to have cancer? 

To provide any answer in terms of frequencies, we need to imagine a specific sample size $N$. As the problem referred to a population of $1000$ women, we conveniently pick a sample size of $N = 1000$ women with identical characteristics (which is suggested by mentioning a "representative" sample) and ask: How many women with a positive test result actually have cancer?^[The actual sample size $N$ chosen is irrelevant, but the numbers are easier to calculate when $N$ is a round number and at least as large as the frequencies mentioned in the problem.] 

In this new sample, the frequency of women with cancer and with a positive test result should match the numbers of the original sample. Hence, we can assume that $10$ out of $1000$ women have cancer (`prev`) and $8$ of the $10$ women with cancer receive a positive test resul (`sens`). Importantly, $95$ out of the $990$ women without cancer also receive a positive test result (`fart`). Thus, the number of women with a positive test result is $8 + 95 = 103$, but only $8$ of them actually have cancer. Of course the ratio $\frac{8}{103}$ is identical to our previous probability (of slightly below 7.8\%). 

Incidentally, the reformulation in terms of frequencies protected us from erroneously taking the sensitivity (of `sens` = $\frac{8}{10} = 80\%$) as an estimate of the desired frequency. Whereas it is easy to confuse the term $p( \mathrm{positive\ test}\ |\ \mathrm{cancer} )$ with $p( \mathrm{cancer}\ |\ \mathrm{positive\ test} )$ when the task is expressed in terms of probabilities, it is clearly unreasonable to assume that about 800 of 1000 women (i.e., 80%) actually have cancer (since the prevalence in the population was specified to be 10 in 1000, i.e., 10\%). Thus, reframing the problem in terms of frequencies made us immune against a typical mistake.


### Using `riskyr`

Reframing the probabilistic problem in terms of frequencies made its solution easier. This is neat and probably one of the best tricks in risk literacy education (as advocated by Gigerenzer 2002, 2014). While it is good to have a way of coping with tricky problems, it would be even more desirable to _actually understand_ the interplay between probabilities and frequencies in such tasks and domains. This is where `riskyr` comes into play.

`riskyr` provides a set of basic risk literacy tools in R. As we have seen, the problems humans face when dealing with risk-related information are less of a _computational_, but rather of a _representational_ nature. As a statistical programming language, R is a pretty powerful computational tool, but for our present purposes it is more important that R is also great for designing and displaying aesthetic and informative visualizations. By applying these qualities to the task of training and instruction in risk literacy, `riskyr` is a toolbox that renders risk literacy education more transparent.

Here is how we could address the above problem by using `riskyr`:

#### A. A fancy calculator 

`riskyr` provides a set of functions that allows us to calculate various desired outputs (probabilities and frequencies) 
from given inputs (probabilities and frequencies). For instance, the following function computes the 
positive predictive value `PPV` from the 
3 basic probabilities `prev`, `sens`, and `spec` (with `spec` = 1 -- `fart`) 
that were provided in the original problem:

```{r comp_PPV}
comp_PPV(prev = .01, sens = .80, spec = (1 - .096))
```

It's good to know that `riskyr` can apply Bayes' formula, but so can any other basic calculator --- including by brain on a good day and some environmental support in the form of paper and pencil. The R in `riskyr` only begins to make sense when considering functions like the following:

```{r comp_prob_prob_1}
# Compute probabilities from 3 essential probabilities:                 # Input arguments:
p1 <- comp_prob_prob(prev = .01, sens = .80, spec =   NA, fart = .096)  # prev, sens, NA,   fart
p2 <- comp_prob_prob(prev = .01, sens = .80, spec = .904, fart =   NA)  # prev, sens, spec, NA 
p3 <- comp_prob_prob(prev = .01, sens = .80, spec = .904, fart = .096)  # prev, sens, spec, fart

# Check equality of outputs:
all.equal(p1, p2)
all.equal(p2, p3)
```

The function `comp_prob_prob` computes probabilities from probabilities (hence its name). The probabilities provided need to include a prevalence `prev`, a sensitivity `sens`, and either the specificity `spec` or the false alarm rate `fart` (with `spec` = 1 -- `fart`). The code above shows 3 different ways in which 3 of these "essential" probabilities can be provided (and hence the objects `p1`, `p2`, and `p3` are all equal to each other). 

The probabilities computed by these "essential" probabilities include the `PPV`, which can be obtained by asking for `p1$PPV` = `r p1$PPV`. But the object computed by `comp_prob_prob` is actually a list of 10 probabilities and can be inspected by printing `p1`:

```{r print_p1}
p1
```

The list of probabilities computed includes the 3 essential probabilities (`prev`, `sens`, and `spec` or `fart`) and the desired probability (`p1$PPV` = `r p1$PPV`), but also many other probabilities that may have been asked instead. (See the vignette on [data formats](data_formats.html) for details on these probabilities.)

Incidentally, as R does not case whether probabilities are entered as decimal numbers or fractions, we can check whether the 2nd version of our problem --- the version reframed in terms of frequencies --- yields the same solution:

```{r comp_prob_prob_2}
# Compute probabilities from 3 ratios of frequencies (probabilities):       # Input arguments:
p4 <- comp_prob_prob(prev = 10/1000, sens = 8/10, spec = NA, fart = 95/990) # prev, sens, NA, fart

p4$PPV
```

This shows that the `PPV` computed in this version is only marginally different (`p4$PPV` = `r p4$PPV`). More importantly, it is identical to the ratio $\frac{8}{103}$ = `r 8/103`.


#### B. Translating between formats

Another function of `riskyr` is to translate between representational formats. 
This translation comes in two varieties: 


##### 1. Computing frequencies from probabilities

```{r comp_freq_prob_1}
# Compute frequencies from probabilities:
f1 <- comp_freq_prob(prev =     .01, sens =  .80, spec = NA, fart =   .096, N = 1000)
f2 <- comp_freq_prob(prev = 10/1000, sens = 8/10, spec = NA, fart = 95/990, N = 1000)

# Check equality of outputs:
all.equal(f1, f2)
```

By providing our original probabilities to the function `comp_freq_prob` we can compute a list of frequencies from probabilities (hence the name). To compute frequencies for the specific sample size of 1000 individuals, we need to provide `N = 1000` as an additional argument. As before, it does not matter whether the probabilities are supplied as decimal numbers or as ratios (as long as they actually _are_ probabilities, i.e., numbers from 0 to 1).

As the ratio `fart` = 95/990 is not exactly equal to `fart` = .096 (but rather 95/100 = `r 95/100`) the two versions of our problem actually vary by a bit. Here, the results `f1` and `f2` are only identical because the function `comp_freq_prob` rounds to nearest integers by default. To compute more precise frequencies (that no longer round to integers), use the `round = FALSE` argument:

```{r comp_freq_prob_2}
# Compute frequencies from probabilities (without rounding):
f3 <- comp_freq_prob(prev =     .01, sens =  .80, spec = NA, fart =   .096, N = 1000, round = FALSE)
f4 <- comp_freq_prob(prev = 10/1000, sens = 8/10, spec = NA, fart = 95/990, N = 1000, round = FALSE)

# Check equality of outputs:
all.equal(f3, f4)  # => shows slight differences in some frequencies:
```

As before, the function `comp_freq_prob` does not compute only one frequency, but a list of 9 frequencies. Their names and values can be inspected by printing `f1`:

```{r print_f1}
f1
```

In this list, the sample of `N` = $1000$ women is split into 3 different subgroups. For instance, the $10$ women with cancer appear as `cond.true` cases, whereas the `990` without cancer are listed as `cond.false` cases. The $8$ women with cancer and a positive test result appear as _hits_ `hi` and the 95 women who receive a positive test result without having cancer are listed as _false alarms_ `fa`. (See the vignette on [data formats](data_formats.html) for details on all frequencies.)


##### 2. Computing probabilities from frequencies

A translator between 2 representational formats should work in both directions. 
Consequently, `riskyr` also allows to compute probabilities by providing frequencies: 

```{r comp_prob_freq}
# Compute probabilities from frequencies:
p5 <- comp_prob_freq(hi = 8, mi = 2, fa = 95, cr = 895)
```

If we provide the function `comp_prob_freq` with the 4 frequencies that were listed as `hi`, `mi`, `fa`, and `cr` in our list of frequencies `f1`, the resulting probabilities match our inputs from above:

```{r}
# Check equality of outputs:
all.equal(p5, p4)
```


##### 3. Switching back and forth

More generally, when we translate between formats twice --- first from probabilities to frequencies and then from the resulting frequencies to probabilities --- the original probabilities appear again:

```{r full_circle}
# Pick 3 random probability inputs:
rand.p <- runif(n = 3, min = 0, max = 1)
rand.p

# Translation 1: Compute frequencies from probabilities:
freq <- comp_freq_prob(prev = rand.p[1], sens = rand.p[2], spec = rand.p[3], round = FALSE)  # without rounding!

# Translation 2: Compute probabilities from frequencies:
prob <- comp_prob_freq(hi = freq$hi, mi = freq$mi, fa = freq$fa, cr = freq$cr)

# Verify that results match original probabilities: 
all.equal(prob$prev, rand.p[1])
all.equal(prob$sens, rand.p[2])
all.equal(prob$spec, rand.p[3])
```

Similarly, going full circle from frequencies to probabilities and back returns the original frequencies:

```{r full_circle_2}
# Pick 4 random frequencies:
rand.f <- round(runif(n = 4, min = 0, max = 10^3), 0)
rand.f  
# sum(rand.f)

# Translation 1: Compute probabilities from frequencies:
prob <- comp_prob_freq(hi = rand.f[1], mi = rand.f[2], fa = rand.f[3], cr = rand.f[4])
# prob

# Translation 2: Compute frequencies from probabilities (for the original population size N):
freq <- comp_freq_prob(prev = prob$prev, sens = prob$sens, spec = prob$spec, N = sum(rand.f), round = FALSE)  # without rounding!
# freq

# Verify that results match original frequencies: 
all.equal(freq$hi, rand.f[1])
all.equal(freq$mi, rand.f[2])
all.equal(freq$fa, rand.f[3])
all.equal(freq$cr, rand.f[4])
```

To obtain the same results when translating back and forth between probabilities and frequencies, it is important to switch off rounding when computing frquencies from probabilities with `comp_freq_prob`. Similarly, we need to scale the computed frequencies to the original population size `N` to arrive at the original frequencies. 


#### C. Visualizing relationships between formats and variables

Inspecting the lists of probabilities and frequencies shows that the two problem formulations cited above are only two possible instances out of an array of many alternative formulations. Essentially, the same scenario can be described in a variety of variables and formats. Gaining deeper insights into the interplay between these variables requires a solid understanding of the underlying concepts and their mathematical definitions. To facilitate the development of such an understanding, `riskyr` recruits the power of visual representations and shows the same scenario from a variety of angles and perspectives. It is mostly this graphical functionality that supports `riskyr`'s claim on being a toolbox for rendering risk literacy more transparent. Thus, in addition to being a fancy calculator and a translator between formats, `riskyr` is mostly a machine that turns risk-related information into pretty pictures.

`riskyr` provides many alternative visualizations that depict the same risk-related scenario in the form of different representations. As each type of graphic has its own properties and perspective --- strengths that emphasize or illuminate some particular aspect and weaknesses that hide or obscure others --- the different visualizations are somewhat redundant, yet complement and support each other.

Here are some examples that depict the scenario described above: 


##### Tree diagram

Perhaps the most intuitive visualization of the probability and frequency information in our above scenario is provided by a tree diagram that shows the population and the frequency of subgroups as its nodes and the probabilities as its edges:

```{r plot_tree, fig.width = 7.1, fig.height = 6, fig.show = 'asis', fig.cap = "A tree diagram that applies the provided probabilities and frequencies to a sample of N = 1000 individuals."}
plot_tree(prev = .01, sens = .80, spec = NA, fart = .096, N = 1000, 
          title.lbl = "Mammography screening")
```

Importantly, the `plot_tree` function is called with the same 3 essential probabilities (`prev`, `sens`, and `spec`) and 1 frequency (the number of individuals `N` of our sample or population). But in addition to computing risk-related information (e.g., the number of individuals in each of the 4 subgroups at the 2nd level of the tree), the tree diagram visualizes crucial dependencies and relationships between concepts and quantities. For instance, the diagram illustrates that the number of true positives (`hi`) depends on both the condition's prevalance (`prev`) and the decision's sensitivity (`sens`), or that the decision's specificity `spec` can be expressed and computed as the ratio of the number of true negatives (`cr`) divided by the number of unaffected individuals (`cond.false` cases).

For details and additional options of the `plot_tree` function, see the documentation of `?plot_tree`.


+++ here now +++ 


#### Mosaic plot

A population is split in subgroups. 
Population as square and magnitude of various frequencies and probabilities expressed as relative area sizes:

```{r plot_mosaic, fig.width = 6, fig.height = 5, fig.show = 'asis'}
plot_mosaic(prev = .01, sens = .80, spec =   NA, fart = .096, N = 1000,
            title.lbl = "Mammography screening")
```

For details, see the documentation on `?plot_mosaic`.

#### Network plot

The tree diagram above adopted a particular perspective by splitting the population into 2 subgroups by condition. 

An alternative perspective would ask: How many people are detected as positive vs. negative by the test?

```{r plot_tree_dc, fig.width = 7.1, fig.height = 6, fig.show = 'asis'}
plot_tree(prev = .01, sens = .80, spec =   NA, fart = .096, N = 1000, 
          by = "dc", 
          title.lbl = "Mammography screening",
          dec.pos.lbl = "positive test",
          dec.neg.lbl = "negative test")
```

Generalization of the tree, but adopting 2 perspectives at once: By condition vs. by decision

```{r plot_fnet, fig.width = 7.1, fig.height = 7, fig.show = 'asis'}
plot_fnet(prev = .01, sens = .80, spec =   NA, fart = .096, N = 1000, 
          title.lbl = "Mammography screening")

# init_riskyr(prev = .01, sens = .80, fart = .096, N = 1000)
```

For details, see the documentation on `?plot_fnet`.

```{r all_in_one, fig.width = 7.1, fig.height = 7, fig.show = 'asis'}

# init_riskyr(prev = .01, sens = .80, fart = .096, N = 1000)

```



## References

- Gigerenzer, G. (2002). 
_Reckoning with risk: Learning to live with uncertainty_. 
London, UK: Penguin.

- Gigerenzer, G. (2014). 
_Risk savvy: How to make good decisions_. 
New York, NY: Penguin.

- Gigerenzer, G., Gaissmaier, W., Kurz-Milcke, E., Schwartz, L., & Woloshin, S. (2007). 
Helping doctors and patients make sense of health statistics. 
_Psychological Science in the Public Interest_, _8_, 53--96.

- Gigerenzer, G., & Hoffrage, U. (1995). 
How to improve Bayesian reasoning without instruction: Frequency formats. 
_Psychological Review_, _102_, 684--704.

- Hoffrage, U., Gigerenzer, G., Krauss, S., & Martignon, L. (2002). 
Representation facilitates reasoning: What natural frequencies are and what they are not. 
_Cognition_, _84_, 343--352.

- Hoffrage, U., Krauss, S., Martignon, L., & Gigerenzer, G. (2015). 
Natural frequencies improve Bayesian reasoning in simple and complex inference tasks. 
_Frontiers in Psychology_, _6_, 1473.

- Hoffrage, U., Lindsey, S., Hertwig, R., & Gigerenzer, G. (2000). 
Communicating statistical information. 
_Science_, _290_, 2261--2262.

- Kurzenhäuser, S., & Hoffrage, U. (2002). 
Teaching Bayesian reasoning: An evaluation of a classroom tutorial for medical students. 
_Medical Teacher_, _24_, 516--521.

- Kurz-Milcke, E., Gigerenzer, G., & Martignon, L. (2008). 
Transparency in risk communication. 
_Annals of the New York Academy of Sciences_, _1128_, 18--28.

- Sedlmeier, P., & Gigerenzer, G. (2001). 
Teaching Bayesian reasoning in less than two hours. 
_Journal of Experimental Psychology: General_, _130_, 380--400.


# Material

<!-- quote: --> 

> "Solving a problem simply means representing it so as to make the solution transparent."<br>
> (H.A. Simon)^[Simon, H.A. (1996). _The Sciences of the Artificial_ (3rd ed.). The MIT Press, Cambridge, MA. (p. 132).]






The problems addressed by `riskyr` are less of a computational than of a representational nature. 

Ordinary people and experts struggle alike: aptly-named "confusion matrix".

Measures are simple, but complex in their abundance (see examples) and inter-dependence. 


Our goal: Promoting understanding, fostering transparency. 

The method to our solution is a matter of organization and visualization.


## Probabilities and Frequencies

### Motivation

A basic motivation for developing `riskyr` was to facilitate the understanding of problems concering so-called Bayesian reasoning. Such problems tend to appear in texts and tutorials on risk literacy and are ubiquitous in medical diagnostics. A typical example reads as follows:

(provide example here)

Problems of this type tend to provide three probabilities:

1. the prevalence `prev` of some condition $C$;  
2. the sensitivity `sens` of some decision $D$ (e.g., the outcome of some test or diagnostic procedure); 
3. the specificity `spec` of $D$;

and ask for one or more other probabilities:

- the probability of a positive (or negative) decision; 
- the positive predictive value `PPV` of $D$;  
- the negative predictive value `NPV` of $D$. 

Standard observation: People are confused and perplexed. Frequently, they even find it difficult to understand the question, let alone have a systematic way of answering it. In a majority of cases, they either have no clue what to respond, or provide some number close to the provided values `sens` or `spec` as a rough estimate.

Importantly, the calculations involved are easy -- almost trivial -- and do only require simple sums, products, and ratios. What's lacking is a systematic way of thinking about such problems and to see how probabilistic notions can be defined and be translated into more manageable information formats (frequencies and ratios between frequencies). The main mission of `riskyr` is to help people to understand this interplay of probabilities and frequencies and thus render risk-related information more transparent.

### Probabilities

#### Terminology

The list `prob` contains 10 probabilities: 

1. the condition's prevalence  `prev`:<br>
The probability of condition being `TRUE` (`prev = cond.true/N`).

2. the decision's sensitivity  `sens`:<br>
The conditional probability of a positive decision provided that the condition is `TRUE`.

3. the decision's miss rate  `mirt`:<br>
The conditional probability of a negative decision provided that the condition is `TRUE`).

4. the decision's specificity  `spec`:<br> 
The conditional probability of a negative decision provided that the condition is `FALSE`).

5. the decision's false alarm rate `fart`:<br> 
The conditional probability of a positive decision provided that the condition is `FALSE`).

6. the proportion (baseline probability or rate) of positive decisions `ppod` (but not necessarily true decisions): `ppod = dec.pos/N`.

7. the decision's positive predictive value  `PPV`:<br> 
The conditional probability of the condition being `TRUE` provided that the decision is positive)

8. the decision's negative predictive value  `NPV`:<br> 
The conditional probability of the condition being `FALSE` provided that the decision is negative).

9. the decision's false discovery or false detection rate  `FDR`:<br> 
The conditional probability of the condition being `FALSE` provided that the decision is positive).

10. the decision's false omission rate  `FOR`:<br> 
The conditional probability of the condition being `TRUE` provided that the decision is negative).


Other ways to classify probabilities: 

Non-conditional vs. conditional probabilities.

3 essential (1 non-conditional and 2 conditional) probabilities 
and 7 derived (1 non-conditional and 6 conditional) probabilities:






#### Basic knowledge

Basic knowledge about probabilities:

- Probabilities can be expressed as or defined in terms of _ratios_ (e.g., the probability of it being Monday $p(\mathrm{Monday}) = \frac{1}{7}$). When expressed as a decimal number, probabilites are numbers in the range from 0 to 1 (e.g., $p(\mathrm{Monday}) = \frac{1}{7}~=~$ `r 1/7`.).

- If some event $E$ has a probability $p(E)$, the probability of $not~E$ (sometimes denoted as $\bar{E}$ or $\neg{E}$) is $p(\bar{E}) = 1 - p$ and is called the _complement_ of $p(E)$.

- Conditional probabilities are not reversable.

- Conditional probabilities can be computed with Bayesian formula or in terms of natural frequencies.  The former is difficult, the latter simple and transparent.


### Frequencies

The following frequencies are stored in `freq`:

1. the population size `N`
2. the number of cases for which `cond.true`
3. the number of cases for which `cond.false`
4. the number of cases for which `dec.pos`
5. the number of cases for which `dec.neg`
6. the number true positives, or hits `hi`
7. the number false negatives, or misses `mi`
8. the number false positives, or false alarms `fa`
9. the number true negatives, or correct rejections `cr`


## Template

Vignettes are long form documentation commonly included in packages. Because they are part of the distribution of the package, they need to be as compact as possible. The `html_vignette` output type provides a custom style sheet (and tweaks some options) to ensure that the resulting html is as small as possible. The `html_vignette` format:

- Never uses retina figures
- Has a smaller default figure size
- Uses a custom CSS stylesheet instead of the default Twitter Bootstrap style

## Vignette Info

Note the various macros within the `vignette` section of the metadata block above. These are required in order to instruct R how to build the vignette. Note that you should change the `title` field and the `\VignetteIndexEntry` to match the title of your vignette.

## Styles

The `html_vignette` template includes a basic CSS theme. To override this theme you can specify your own CSS in the document metadata as follows:

    output: 
      rmarkdown::html_vignette:
        css: mystyles.css

## Figures

The figure sizes have been customised so that you can easily put two images side-by-side. 

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```

You can enable figure captions by `fig_caption: yes` in YAML:

    output:
      rmarkdown::html_vignette:
        fig_caption: yes

Then you can use the chunk option `fig.cap = "Your figure caption."` in **knitr**.

## More Examples

You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables, e.g. using `knitr::kable()`.

```{r, echo=FALSE, results='asis'}
knitr::kable(head(mtcars, 10))
```

Also a quote using `>`:

> "He who gives up [code] safety for [code] speed deserves neither."
([via](https://twitter.com/hadleywickham/status/504368538874703872))
